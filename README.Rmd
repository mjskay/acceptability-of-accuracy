# Code for _How good is 85%? A survey tool to connect classifier evaluation to acceptability of accuracy_

<a rel="license" href="http://creativecommons.org/licenses/by/4.0/"><img
alt="This work is licensed under a Creative Commons Attribution 4.0 International License" 
title="This work is licensed under a Creative Commons Attribution 4.0 International License" 
src="https://i.creativecommons.org/l/by/4.0/88x31.png" /></a> _Matthew Kay ([mjskay@uw.edu]
(mailto:mjskay@uw.edu)), Shwetak N. Patel ([shwetak@uw.edu]
(mailto:shwetak@uw.edu)), and Julie A. Kientz ([jkientz@uw.edu]
(mailto:jkientz@uw.edu))_

This repository contains analysis code from: 

Kay, Matthew, Patel, Shwetak N., and Kientz, Julie A. How Good is 85%? A Survey 
Tool to Connect Classifier Evaluation to Acceptability of Accuracy. _CHI 2015_
(upcoming). http://dx.doi.org/10.1145/2702123.2702603

It is intended to allow others to adopt our tool for generating surveys and
modelling acceptability of accuracy. It is currently a work-in-progress. If you
have any questions, please email Matthew Kay (above). Also, if you've done 
something cool with this work, we'd love to hear from you!

## Libraries needed for these analyses

```{r setup, include=FALSE}
knitr::opts_chunk$set(cache=TRUE)           #default code chunk options
pander::panderOptions("table.split.table", Inf)     #don't split wide tables in output
pander::panderOptions("table.style", "rmarkdown")   #table style that's supported by github
```

```{r libraries, message=FALSE, warning=FALSE, cache=FALSE}
library(runjags)
library(tidybayes)
library(metajags)
library(coda)
library(lme4)
library(plyr)
library(dplyr)
library(ggplot2)
library(pander)
```

Plus some helper functions in [util.R](util.R):

```{r util}
source("util.R")
```

## Example model

### Fitting the model
Example code for fitting a model can be found in [src/application_ui-regression.R](src/application_ui-regression.R). Since fitting 
the model takes some time, for the purposes of this example we will just load the fitted model, 
which has been saved in `src/output/acceptability_ui-model-small-final.RData`:

```{r load_model}
load(file="output/acceptability_ui-model-small-final.RData")
```

### Parameter estimates

Let's plot some posterior parameter estimates. First, we extract the 
sample estimates for `b0`, `b`, and `alpha`:

```{r extract_params, warnings=FALSE}
params = extract_samples(best_model_chain, cbind(b0, b, alpha)[application])
```

This gives us a pretty simple table of estimates. We can look at the
first couple of entries to get an idea of its structure:

```{r params_table_1, eval=FALSE}
head(params)
```

```{r params_table_2, echo=FALSE, results='asis'}
pander(head(params))
```

Now we'll plot each parameter in turn, with some useful reference lines:

```{r params_plot}
ggposterior(params, aes(x=application, y=b)) +
    geom_hline(yintercept=0, lty="dashed")

ggposterior(params, aes(x=application, y=b0)) +
    geom_hline(yintercept=0, lty="dashed")

ggposterior(params, aes(x=application, y=alpha)) +
    geom_hline(yintercept=0.5, lty="dashed") +
    ylim(0, 1)
```

### Differences in alpha

We can also examine the posterior difference in alpha between each condition. First
we extract the samples for alpha, this time in a wide format to facilitate comparison:

```{r extract_alpha}
alpha = extract_samples(best_model_chain, alpha[application] | application)
```

Again, let's see the first couple of entries to get an idea of its structure:

```{r alpha_table_1, eval=FALSE}
head(alpha)
```

```{r alpha_table_2, echo=FALSE, results='asis'}
pander(head(alpha))
```

Now, for every pair of applications, let's get the posterior distribution
of their difference in alpha:

```{r alpha_diff}
alpha_comparisons = ldply(combn(levels(df$application), 2, simplify=FALSE), 
    function(applications) {
        data.frame(
            applications = paste(applications, collapse=" - "), 
            alpha_difference = alpha[[applications[[1]]]] - alpha[[applications[[2]]]]
        ) 
    })
```

Which looks like this:

```{r alpha_comp_table_1, eval=FALSE}
head(alpha_comparisons)
```

```{r alpha_comp_table_2, echo=FALSE, results='asis'}
pander(head(alpha_comparisons))
```

Finally, we can plot the estimated differences:

```{r alpha_comparison_plot}
ggposterior(alpha_comparisons, aes(x=applications, y=alpha_difference)) + 
    geom_hline(yintercept=0, lty="dashed") +
    ylim(-0.5, 0.5)
```

## Citing this work

Please cite the CHI paper above.

## Problems

Should you encounter any issues with this code, contact Matthew Kay
(<mjskay@uw.edu>). If you have found a bug, please file it [here](https://github.com/mjskay/acceptability-of-accuracy/issues/new) with minimal code to reproduce
the issue.


